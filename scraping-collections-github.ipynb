{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Scraping top 20 collections from GitHub Collections page and its top repositories\n\n## The project is divided into two tasks: \n##### 1. Create a .csv file for the list of collections on GitHub page in the format of | Title | Description | URL |\n##### 2. Create individual .csv files for all the repositories of an each collection following this format: \n|username | repo_name | repo_desc | stars | forks | programming language | repo_url |\n","metadata":{}},{"cell_type":"markdown","source":"## 1. Create a .csv file for the list of collections on GitHub page: \n> Website URL I am scraping:  https://github.com/collections","metadata":{}},{"cell_type":"markdown","source":"### Step 1: Downloading the github collections page\n* use requests lib to download the page\n* use BS4 to parse and extract the page ","metadata":{}},{"cell_type":"code","source":"import requests \nfrom bs4 import BeautifulSoup\n\ndef get_collections_page():\n    collections_url = 'https://github.com/collections'\n    response = requests.get(collections_url)\n    if response.status_code != 200:\n        raise Exception ('Failed to load {} page.'.format(collections_url))\n    doc =  BeautifulSoup(response.text, 'html.parser')\n    return doc","metadata":{"execution":{"iopub.status.busy":"2021-10-01T16:05:54.959250Z","iopub.execute_input":"2021-10-01T16:05:54.959819Z","iopub.status.idle":"2021-10-01T16:05:55.303047Z","shell.execute_reply.started":"2021-10-01T16:05:54.959701Z","shell.execute_reply":"2021-10-01T16:05:55.301672Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"doc = get_collections_page()","metadata":{"execution":{"iopub.status.busy":"2021-10-01T16:05:55.305101Z","iopub.execute_input":"2021-10-01T16:05:55.305490Z","iopub.status.idle":"2021-10-01T16:05:55.905551Z","shell.execute_reply.started":"2021-10-01T16:05:55.305454Z","shell.execute_reply":"2021-10-01T16:05:55.904624Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### STEP 2: Creating individual helper functions for fetching Collections' Titles, Descriptions and URLs","metadata":{}},{"cell_type":"code","source":"# Fetching Titles\ndef get_collection_titles(doc): \n    selection_class = 'h3'\n    collection_titles_tag = doc.find_all( 'h2', {'class': selection_class}) \n    collection_titles = []\n    for tag in collection_titles_tag:\n        collection_titles.append(tag.text)\n    return collection_titles","metadata":{"execution":{"iopub.status.busy":"2021-10-01T16:05:55.907442Z","iopub.execute_input":"2021-10-01T16:05:55.908026Z","iopub.status.idle":"2021-10-01T16:05:55.912922Z","shell.execute_reply.started":"2021-10-01T16:05:55.907966Z","shell.execute_reply":"2021-10-01T16:05:55.912117Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"titles = get_collection_titles(doc)\nlen(titles)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T16:05:55.914456Z","iopub.execute_input":"2021-10-01T16:05:55.914939Z","iopub.status.idle":"2021-10-01T16:05:55.932967Z","shell.execute_reply.started":"2021-10-01T16:05:55.914888Z","shell.execute_reply":"2021-10-01T16:05:55.932124Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Fetching Descriptions\ndef get_collection_desc(doc):\n    desc_selector = 'col-10 col-md-11'\n    collections_desc_tag = doc.find_all('div', {'class': desc_selector})\n    temp_collection_desc = []\n    for tag in collections_desc_tag:\n        temp_collection_desc.append(tag.text.strip())\n    collection_desc = []\n    for i in temp_collection_desc:\n        whole_collection = [i.split('\\n')[1].strip()]\n        collection_desc += whole_collection\n    return collection_desc","metadata":{"execution":{"iopub.status.busy":"2021-10-01T16:05:55.934253Z","iopub.execute_input":"2021-10-01T16:05:55.934762Z","iopub.status.idle":"2021-10-01T16:05:55.945867Z","shell.execute_reply.started":"2021-10-01T16:05:55.934710Z","shell.execute_reply":"2021-10-01T16:05:55.944663Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"desc = get_collection_desc(doc)\nlen(desc)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T16:05:55.947822Z","iopub.execute_input":"2021-10-01T16:05:55.948335Z","iopub.status.idle":"2021-10-01T16:05:55.966422Z","shell.execute_reply.started":"2021-10-01T16:05:55.948281Z","shell.execute_reply":"2021-10-01T16:05:55.965181Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Fetching URLs\ndef get_collection_urls(doc):\n    urls = []\n    for tag in doc.find_all('a', {'href': True, 'class': None}):\n        if tag['href'].startswith('/collections/') == True:\n            each_urls = ['https://github.com' + tag['href']]\n            urls += each_urls\n    return urls","metadata":{"execution":{"iopub.status.busy":"2021-10-01T16:05:55.969814Z","iopub.execute_input":"2021-10-01T16:05:55.970300Z","iopub.status.idle":"2021-10-01T16:05:55.979086Z","shell.execute_reply.started":"2021-10-01T16:05:55.970222Z","shell.execute_reply":"2021-10-01T16:05:55.977687Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"collections_urls = get_collection_urls(doc)\nlen(collections_urls)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T16:05:55.980691Z","iopub.execute_input":"2021-10-01T16:05:55.981245Z","iopub.status.idle":"2021-10-01T16:05:55.996396Z","shell.execute_reply.started":"2021-10-01T16:05:55.981196Z","shell.execute_reply":"2021-10-01T16:05:55.995439Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### STEP 3: Putting everything together to scrape a final .csv file containing list of top 20 collections","metadata":{}},{"cell_type":"code","source":"import pandas as pd","metadata":{"execution":{"iopub.status.busy":"2021-10-01T16:05:55.999350Z","iopub.execute_input":"2021-10-01T16:05:55.999836Z","iopub.status.idle":"2021-10-01T16:05:56.006381Z","shell.execute_reply.started":"2021-10-01T16:05:55.999787Z","shell.execute_reply":"2021-10-01T16:05:56.005545Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Scraping the collections .csv\ndef scrape_collections():\n    collections_url = 'https://github.com/collections'\n    response = requests.get(collections_url)\n    if response.status_code != 200:\n        raise Exception ('Failed to load {} page.'.format(collections_url))\n    doc =  BeautifulSoup(response.text, 'html.parser')\n    collections_dict = {\n        'Title': get_collection_titles(doc),\n        'Description': get_collection_desc(doc),\n        'URL': get_collection_urls(doc)\n    }\n    return pd.DataFrame(collections_dict)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T16:05:56.008097Z","iopub.execute_input":"2021-10-01T16:05:56.008576Z","iopub.status.idle":"2021-10-01T16:05:56.018320Z","shell.execute_reply.started":"2021-10-01T16:05:56.008529Z","shell.execute_reply":"2021-10-01T16:05:56.017138Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"collections = scrape_collections()\ncollections[:5]","metadata":{"execution":{"iopub.status.busy":"2021-10-01T16:05:56.019698Z","iopub.execute_input":"2021-10-01T16:05:56.020081Z","iopub.status.idle":"2021-10-01T16:05:56.567623Z","shell.execute_reply.started":"2021-10-01T16:05:56.020049Z","shell.execute_reply":"2021-10-01T16:05:56.566488Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Downloading the final .csv file\ncollections.to_csv('collections.csv', index = None)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T16:05:56.568962Z","iopub.execute_input":"2021-10-01T16:05:56.569311Z","iopub.status.idle":"2021-10-01T16:05:56.577586Z","shell.execute_reply.started":"2021-10-01T16:05:56.569278Z","shell.execute_reply":"2021-10-01T16:05:56.576243Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## END OF TASK 1","metadata":{"execution":{"iopub.status.busy":"2021-10-01T15:30:12.961034Z","iopub.execute_input":"2021-10-01T15:30:12.961378Z","iopub.status.idle":"2021-10-01T15:30:12.964879Z","shell.execute_reply.started":"2021-10-01T15:30:12.961349Z","shell.execute_reply":"2021-10-01T15:30:12.964199Z"}}},{"cell_type":"markdown","source":"## 2. Create individual .csv files for all the repositories of an each collection following this format: \n|username | repo_name | repo_desc | stars | forks | programming language | repo_url |","metadata":{}},{"cell_type":"markdown","source":"### STEP 1: Downloading all collection pages using its URLs   ","metadata":{}},{"cell_type":"code","source":"# First thing in pipeline is to download individual collection page\ndef get_collection_page(collection_url):\n    response = requests.get(collection_url)\n    if response.status_code != 200:\n        raise Exception('Failed to load {} page'.format(collection_url))\n    # Parse the collection page using BS4\n    collection_doc = BeautifulSoup(response.text, 'html.parser')\n    return collection_doc","metadata":{"execution":{"iopub.status.busy":"2021-10-01T16:05:56.578927Z","iopub.execute_input":"2021-10-01T16:05:56.579265Z","iopub.status.idle":"2021-10-01T16:05:56.586926Z","shell.execute_reply.started":"2021-10-01T16:05:56.579224Z","shell.execute_reply":"2021-10-01T16:05:56.585779Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Rough Work","metadata":{}},{"cell_type":"code","source":"# topic_doc = get_collection_page('https://github.com/collections/clean-code-linters')\n# desc_tag = topic_doc.findAll('div', {'class': 'color-text-secondary mb-2 ws-normal'})\n# items = topic_doc.findAll(\"div\", {'class': 'color-text-secondary mb-2 ws-normal'})\n# # second_child = first_child.find('span', {'class': 'ml-0'})\n# for desc in desc_tag:\n#     print(desc.text.strip())\n    \n    \n\n        \n# results = list(zip(name,tag))\n# df = pd.DataFrame(results)\n            ","metadata":{"execution":{"iopub.status.busy":"2021-10-01T16:05:56.588236Z","iopub.execute_input":"2021-10-01T16:05:56.588547Z","iopub.status.idle":"2021-10-01T16:05:56.599345Z","shell.execute_reply.started":"2021-10-01T16:05:56.588515Z","shell.execute_reply":"2021-10-01T16:05:56.598350Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### STEP 2: Creating a function for getting the following repository details:\nusername, repo_name, repo_desc, stars, forks, programming language, repo_url","metadata":{}},{"cell_type":"code","source":"# Getting Repo Info.\ndef get_repo_info(h1_tag, star_tag, fork_tag, lan_tag, desc_tag):\n    a_tags = h1_tag.find_all('a')\n    username = a_tags[0].text.strip().split('\\n')[0][:-2]\n    repo_name = a_tags[0].text.strip().split('\\n')[1].strip()\n    repo_url = 'https://www.github.com' + a_tags[0]['href']\n    \n    # Languages tag filter\n    lan = []\n    desc = []\n\n#     for each in lan_tag:\n    try:\n        lan.append(lan_tag.find('span',{'class':'ml-0'}).text.strip())\n    except:\n        lan.append('Not Provided')\n    \n    try:\n        desc.append(desc_tag.find('div',{'class':'color-text-secondary mb-2 ws-normal'}).text.strip())\n    except:\n        desc.append('Not Provided')\n    \n    # Stars and Forks tag filter\n    stars = int(star_tag.text.strip())\n    forks = int(fork_tag.text.strip())\n   \n    \n    return username, repo_name, desc[0], stars, forks, lan[0], repo_url","metadata":{"execution":{"iopub.status.busy":"2021-10-01T16:05:56.601397Z","iopub.execute_input":"2021-10-01T16:05:56.601834Z","iopub.status.idle":"2021-10-01T16:05:56.613409Z","shell.execute_reply.started":"2021-10-01T16:05:56.601788Z","shell.execute_reply":"2021-10-01T16:05:56.612208Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### STEP 3: Creating the final function for scraping details of every collection's all repositories using get_repo_info() function.","metadata":{}},{"cell_type":"code","source":"import re","metadata":{"execution":{"iopub.status.busy":"2021-10-01T16:05:56.615345Z","iopub.execute_input":"2021-10-01T16:05:56.615737Z","iopub.status.idle":"2021-10-01T16:05:56.628737Z","shell.execute_reply.started":"2021-10-01T16:05:56.615704Z","shell.execute_reply":"2021-10-01T16:05:56.627519Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# put everything in final function\ndef get_collection_repo(collection_doc):\n    h1_selection_class = 'h3 lh-condensed'\n    repo_tag = collection_doc.find_all('h1', {'class': h1_selection_class})\n    star_tag = collection_doc.find_all(href = re.compile('stargazers'))\n    fork_tag = collection_doc.find_all(href = re.compile('members'))\n    lan_tag = collection_doc.findAll(\"div\", {'class': 'd-flex f6'})\n    desc_tag = collection_doc.findAll('article', {'class': 'height-full border color-border-secondary rounded-1 p-3 p-md-5 my-5'})\n    \n    collection_dict = {'username': [], 'repo_name': [], 'repo_desc': [], 'stars': [], 'forks': [], 'programming language': [],'repo_url': []}\n    \n    for i in range(len(repo_tag)):\n        repo_info = get_repo_info(repo_tag[i], star_tag[i], fork_tag[i], lan_tag[i], desc_tag[i])\n#         starforks = get_stars_forks(topic_doc)\n        collection_dict['username'].append(repo_info[0])\n        collection_dict['repo_name'].append(repo_info[1])\n        collection_dict['repo_desc'].append(repo_info[2])\n        collection_dict['stars'].append(repo_info[3])\n        collection_dict['forks'].append(repo_info[4])\n        collection_dict['programming language'].append(repo_info[5])\n        collection_dict['repo_url'].append(repo_info[6])\n        \n    return pd.DataFrame(collection_dict, index = None)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T16:05:56.630748Z","iopub.execute_input":"2021-10-01T16:05:56.631320Z","iopub.status.idle":"2021-10-01T16:05:56.644163Z","shell.execute_reply.started":"2021-10-01T16:05:56.631265Z","shell.execute_reply":"2021-10-01T16:05:56.642866Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Let's now start scraping .csv for each collection's repositories\n\ndef scrape_collection(collection_url, path):\n    if os.path.exists(path):\n        print('The file {} already exists. Skipping...'.format(path))\n        return\n    collection_df = get_collection_repo(get_collection_page(collection_url))\n    collection_df.to_csv(path, index = None)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T16:05:56.646393Z","iopub.execute_input":"2021-10-01T16:05:56.647072Z","iopub.status.idle":"2021-10-01T16:05:56.660414Z","shell.execute_reply.started":"2021-10-01T16:05:56.647024Z","shell.execute_reply":"2021-10-01T16:05:56.659530Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"### STEP 4: Putting everything together and creating a function that gives the .csvs for each collection repos","metadata":{}},{"cell_type":"code","source":"import os","metadata":{"execution":{"iopub.status.busy":"2021-10-01T16:05:56.661394Z","iopub.execute_input":"2021-10-01T16:05:56.661675Z","iopub.status.idle":"2021-10-01T16:05:56.674168Z","shell.execute_reply.started":"2021-10-01T16:05:56.661648Z","shell.execute_reply":"2021-10-01T16:05:56.673293Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Let's now start scraping .csv for each collection's repositories\n\ndef scrape_collection_repos():\n    print(\"Scraping List of Collections\")\n    collection_df = scrape_collections()\n    \n    os.makedirs('data', exist_ok = True)\n    \n    for index, row in collection_df.iterrows():\n        print('Scraping top repositories for {}'.format(row['Title']))\n        scrape_collection(row['URL'], 'data/{}.csv'.format(row['Title']))","metadata":{"execution":{"iopub.status.busy":"2021-10-01T16:05:56.675166Z","iopub.execute_input":"2021-10-01T16:05:56.675477Z","iopub.status.idle":"2021-10-01T16:05:56.688500Z","shell.execute_reply.started":"2021-10-01T16:05:56.675446Z","shell.execute_reply":"2021-10-01T16:05:56.687450Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"scrape_collection_repos()","metadata":{"execution":{"iopub.status.busy":"2021-10-01T16:05:56.689842Z","iopub.execute_input":"2021-10-01T16:05:56.690165Z","iopub.status.idle":"2021-10-01T16:06:13.587713Z","shell.execute_reply.started":"2021-10-01T16:05:56.690134Z","shell.execute_reply":"2021-10-01T16:06:13.586661Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"## Check the .csvs\n#pd.read_csv('./data/DevOps tools.csv')","metadata":{"execution":{"iopub.status.busy":"2021-10-01T16:06:13.591865Z","iopub.execute_input":"2021-10-01T16:06:13.592224Z","iopub.status.idle":"2021-10-01T16:06:13.596193Z","shell.execute_reply.started":"2021-10-01T16:06:13.592191Z","shell.execute_reply":"2021-10-01T16:06:13.595312Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"## END OF TASK 2","metadata":{}},{"cell_type":"markdown","source":"## Summary:\n* I've learnt various things such as parsing data using BS4, creating several helper functions and putting all functions together to make the code look organized and less confusing.\n* I've learnt ways to figure out all kinds of problems while executing each steps by myself and gained confidence for been able to work my way in achieving goals.\n* I've understood the significance of web scrapping and enjoyed the creativity of an automating systems\n\nTODO: \n\n* explain what you learnt, what you used, and what would be the future work of this project.","metadata":{}},{"cell_type":"markdown","source":"## Future Work:\n* Instead of scraping top 20 collections, I can go ahead and scrape all collections repo .csvs and create a .py file.\n* Obviously an automated scarping tool can be created with a simple front-end which contains the search box for users to give input and fetch the matching .csv files based on what is searched from the whole list of collections available on GitHub and deploy the project globally.\n* Another step ahead could be to scrape the whole Github collections, topics, trending, pages and make an end to end project on complete libraries. \n* Similar web scrapping projects can be created using scrappy and other tools.","metadata":{}}]}